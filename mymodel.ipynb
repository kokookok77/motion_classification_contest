{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "train=pd.read_csv('train_features.csv')\n",
    "train_labels=pd.read_csv('train_labels.csv')\n",
    "test=pd.read_csv('test_features.csv')\n",
    "submission=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'label' in train.columns:\n",
    "    train.drop(columns=['label'], inplace=True)\n",
    "train_total = pd.merge(train, train_labels, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_labels['label'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1875000\n1875000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_total))\n",
    "print(len(train_labels)*600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "37: 35\n",
      "37: 455\n",
      "37\n",
      "26\n",
      "3: 23\n",
      "3: 299\n",
      "3\n",
      "39: 20\n",
      "39: 260\n",
      "39\n",
      "28: 55\n",
      "28\n",
      "6: 24\n",
      "6: 312\n",
      "6\n",
      "60: 48\n",
      "60: 624\n",
      "60\n",
      "35: 30\n",
      "35: 390\n",
      "35\n",
      "34: 22\n",
      "34: 286\n",
      "34\n",
      "30: 35\n",
      "30: 455\n",
      "30\n",
      "59: 23\n",
      "59: 299\n",
      "59\n",
      "15: 25\n",
      "15: 325\n",
      "15\n",
      "40: 34\n",
      "40: 442\n",
      "40\n",
      "50: 37\n",
      "50: 481\n",
      "50\n",
      "48: 25\n",
      "48: 325\n",
      "48\n",
      "8: 97\n",
      "8\n",
      "54: 23\n",
      "54: 299\n",
      "54\n",
      "16: 22\n",
      "16: 286\n",
      "16\n",
      "4: 35\n",
      "4: 455\n",
      "4\n",
      "58: 20\n",
      "58: 260\n",
      "58\n",
      "21: 27\n",
      "21: 351\n",
      "21\n",
      "18: 47\n",
      "18: 611\n",
      "18\n",
      "51: 24\n",
      "51: 312\n",
      "51\n",
      "25: 24\n",
      "25: 312\n",
      "25\n",
      "24: 35\n",
      "24: 455\n",
      "24\n",
      "17: 27\n",
      "17: 351\n",
      "17\n",
      "49: 30\n",
      "49: 390\n",
      "49\n",
      "47: 26\n",
      "47: 338\n",
      "47\n",
      "27: 34\n",
      "27: 442\n",
      "27\n",
      "36: 28\n",
      "36: 364\n",
      "36\n",
      "2: 20\n",
      "2: 260\n",
      "2\n",
      "38: 20\n",
      "38: 260\n",
      "38\n",
      "14: 25\n",
      "14: 325\n",
      "14\n",
      "10: 20\n",
      "10: 260\n",
      "10\n",
      "55: 37\n",
      "55: 481\n",
      "55\n",
      "57: 20\n",
      "57: 260\n",
      "57\n",
      "0: 12\n",
      "0: 156\n",
      "0\n",
      "1: 21\n",
      "1: 273\n",
      "1\n",
      "46: 20\n",
      "46: 260\n",
      "46\n",
      "43: 35\n",
      "43: 455\n",
      "43\n",
      "52: 12\n",
      "52: 156\n",
      "52\n",
      "53: 13\n",
      "53: 169\n",
      "53\n",
      "5: 25\n",
      "5: 325\n",
      "5\n",
      "56: 36\n",
      "56: 468\n",
      "56\n",
      "23: 20\n",
      "23: 260\n",
      "23\n",
      "44: 21\n",
      "44: 273\n",
      "44\n",
      "42: 20\n",
      "42: 260\n",
      "42\n",
      "19: 20\n",
      "19: 260\n",
      "19\n",
      "32: 18\n",
      "32: 234\n",
      "32\n",
      "45: 22\n",
      "45: 286\n",
      "45\n",
      "29: 20\n",
      "29: 260\n",
      "29\n",
      "31: 20\n",
      "31: 260\n",
      "31\n",
      "11: 23\n",
      "11: 299\n",
      "11\n",
      "7: 26\n",
      "7: 338\n",
      "7\n",
      "41: 20\n",
      "41: 260\n",
      "41\n",
      "12: 12\n",
      "12: 156\n",
      "12\n",
      "9: 37\n",
      "9: 481\n",
      "9\n",
      "20: 26\n",
      "20: 338\n",
      "20\n",
      "13: 12\n",
      "13: 156\n",
      "13\n",
      "22: 19\n",
      "22: 247\n",
      "22\n",
      "33: 20\n",
      "33: 260\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "aug_cnt = 700\n",
    "\n",
    "for idx, label in labels.iteritems():\n",
    "    if os.path.exists(f'./aug/train_labels_{label}.csv'):\n",
    "        print(f'./aug/train_labels_{label}.csv', ' is already exists!')\n",
    "    else :\n",
    "        tmps = train_total[train_total['label'] == label].copy()\n",
    "        while len(train_labels[train_labels['label'] == label]) < aug_cnt:\n",
    "            id_list = tmps['id'].drop_duplicates()\n",
    "            print(f'{label}:', len(train_labels[train_labels['label'] == label]))\n",
    "            for _, _id in id_list.iteritems(): \n",
    "                if len(train_labels[train_labels['label'] == label]) < aug_cnt:\n",
    "                    tmp = tmps[tmps['id'] == _id].copy()\n",
    "                    tmp['acc_x'] = tmp['acc_x'] + tmp['acc_x'].std() * np.random.normal(0.0, 1.0, 600)\n",
    "                    tmp['acc_y'] = tmp['acc_y'] + tmp['acc_y'].std() * np.random.normal(0.0, 1.0, 600)\n",
    "                    tmp['acc_z'] = tmp['acc_z'] + tmp['acc_z'].std() * np.random.normal(0.0, 1.0, 600)\n",
    "                    tmp['gy_x'] = tmp['gy_x'] + tmp['gy_x'].std() * np.random.normal(0.0, 1.0, 600)\n",
    "                    tmp['gy_y'] = tmp['gy_y'] + tmp['gy_y'].std() * np.random.normal(0.0, 1.0, 600)\n",
    "                    tmp['gy_z'] = tmp['gy_z'] + tmp['gy_z'].std() * np.random.normal(0.0, 1.0, 600)\n",
    "                    newid = train_labels['id'].max() + 1\n",
    "                    tmp['id'] = newid\n",
    "                    tmps = pd.concat([tmps, tmp])\n",
    "                    train_labels = train_labels.append({'id': newid, 'label': label}, ignore_index=True)\n",
    "\n",
    "                if len(train_labels[train_labels['label'] == label]) < aug_cnt:\n",
    "                    for rate in [0.1*i for i in range(5, 15)]:\n",
    "                        tmp = tmps[tmps['id'] == _id].copy()\n",
    "                        tmp['acc_x'] = tmp['acc_x']*rate\n",
    "                        tmp['acc_y'] = tmp['acc_y']*rate\n",
    "                        tmp['acc_z'] = tmp['acc_z']*rate\n",
    "                        tmp['gy_x'] = tmp['gy_x']*rate\n",
    "                        tmp['gy_y'] = tmp['gy_y']*rate\n",
    "                        tmp['gy_z'] = tmp['gy_z']*rate\n",
    "                        newid = train_labels['id'].max() + 1\n",
    "                        tmp['id'] = newid\n",
    "                        tmps = pd.concat([tmps, tmp])\n",
    "                        train_labels = train_labels.append({'id': newid, 'label': label}, ignore_index=True)\n",
    "\n",
    "                if len(train_labels[train_labels['label'] == label]) < aug_cnt:\n",
    "                    tmp = tmps[tmps['id'] == _id].copy()\n",
    "                    tmp.sort_values(by='time', ascending=False)\n",
    "                    newid = train_labels['id'].max() + 1\n",
    "                    tmp['id'] = newid\n",
    "                    tmps = pd.concat([tmps, tmp])\n",
    "                    train_labels = train_labels.append({'id': newid, 'label': label}, ignore_index=True)\n",
    "    \n",
    "        print(label)\n",
    "        tmps.to_csv(f'./aug/train_features_{label}.csv')\n",
    "        train_labels[train_labels['label'] == label].to_csv(f'./aug/train_labels_{label}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_labels=pd.read_csv('train_labels.csv')\n",
    "labels = train_labels['label'].drop_duplicates()\n",
    "\n",
    "train_feature = pd.DataFrame()\n",
    "train_label = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "for _, label in labels.iteritems():\n",
    "    tmp_feature = pd.read_csv(f\"aug/train_features_{label}.csv\")\n",
    "    tmp_feature = tmp_feature.iloc[:int(len(tmp_feature)/600)*600,:]\n",
    "    train_feature = pd.concat([train_feature, tmp_feature])\n",
    "\n",
    "    tmp_label = pd.read_csv(f\"aug/train_labels_{label}.csv\").iloc[:int(len(tmp_feature)/600),:]\n",
    "    train_label = pd.concat([train_label, tmp_label])\n",
    "    print(i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature.to_csv('train_feature_aug.csv')\n",
    "train_label.to_csv('train_label_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_feature = pd.read_csv('sampled_feature_2.csv')\n",
    "train_label = pd.read_csv('sampled_label_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_func(data, sample_cnt):\n",
    "    np.random.seed(123)\n",
    "    N = len(data)\n",
    "    sample_n = sample_cnt #int(len(data)*sample_pct) # integer\n",
    "    sample = data.take(np.random.permutation(N)[:sample_n])\n",
    "    return sample\n",
    "\n",
    "\n",
    "sampled_label = train_label.groupby(by='label').apply(sampling_func, sample_cnt=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"id in {list(sampled_label['id'].to_numpy())}\"\n",
    "sampled_feature = train_feature.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_feature.to_csv('sampled_feature_2.csv')\n",
    "sampled_label.to_csv('sampled_label_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(24400, 61)\n"
     ]
    }
   ],
   "source": [
    "y=tf.keras.utils.to_categorical(train_label['label']) \n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(24400, 600, 6)\n"
     ]
    }
   ],
   "source": [
    "X=tf.reshape(np.array(train_feature.iloc[:,3:9]),[-1, 600, 6])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_filters=32, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_filters = num_filters\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def _default_Conv1D(self, filters, kernel_size):\n",
    "        return tf.keras.layers.Conv1D(filters=filters,\n",
    "                                      kernel_size=kernel_size,\n",
    "                                      strides=1,\n",
    "                                      padding='same',\n",
    "                                      activation='relu',\n",
    "                                      use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        z_bottleneck = self._default_Conv1D(filters=self.num_filters, kernel_size=1)(inputs)\n",
    "        z_maxpool = tf.keras.layers.MaxPool1D(pool_size=3, strides=1, padding='same')(inputs)\n",
    "\n",
    "        z1 = self._default_Conv1D(filters=self.num_filters, kernel_size=10)(z_bottleneck)\n",
    "        z2 = self._default_Conv1D(filters=self.num_filters, kernel_size=20)(z_bottleneck)\n",
    "        z3 = self._default_Conv1D(filters=self.num_filters, kernel_size=40)(z_bottleneck)\n",
    "        z4 = self._default_Conv1D(filters=self.num_filters, kernel_size=1)(z_maxpool)\n",
    "\n",
    "        z = tf.keras.layers.Concatenate(axis=2)([z1, z2, z3, z4])\n",
    "        z = tf.keras.layers.BatchNormalization()(z)\n",
    "\n",
    "        return self.activation(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortcut_layer(inputs, z_inception):\n",
    "    z_shortcut = tf.keras.layers.Conv1D(filters=int(z_inception.shape[-1]), \n",
    "                                        kernel_size=1, \n",
    "                                        padding='same', \n",
    "                                        use_bias=False)(inputs)\n",
    "\n",
    "    z_shortcut = tf.keras.layers.BatchNormalization()(z_shortcut)\n",
    "\n",
    "    z = tf.keras.layers.Add()([z_shortcut, z_inception])\n",
    "\n",
    "    return tf.keras.layers.Activation('relu')(z)\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_classes, num_modules=6):\n",
    "    input_layer = tf.keras.layers.Input(input_shape)\n",
    "    z = input_layer\n",
    "    z_residual = input_layer\n",
    "\n",
    "    for i in range(num_modules):\n",
    "        z = InceptionModule()(z)\n",
    "        if i%3 == 2:\n",
    "            z = shortcut_layer(z_residual, z)\n",
    "            z_residual = z\n",
    "\n",
    "    gap_layer = tf.keras.layers.GlobalAveragePooling1D()(z)\n",
    "    output_layer = tf.keras.layers.Dense(num_classes, activation='softmax')(gap_layer)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs= input_layer, outputs= output_layer)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model((600, 6), 61, num_modules=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y, validation_split=0.2, epochs=100, batch_size=128)"
   ]
  }
 ]
}